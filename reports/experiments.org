 #+BIND: org-export-use-babel nil
#+TITLE: Experiments
#+AUTHOR: Pim Meerdink
#+EMAIL: <pimmeerdink@hotmail.com>
#+DATE: January 8, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session experiments :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex


* Introduction

This file describes the experiments that were preformed in the paper. As such,
it also describes the experiments that we will need to replicate.

In general, assesment is done through training two classifiers
1) maps from z_T to T (dubbed the /target predictor/)
1) maps from z_T to S (same architecture as the discriminator)
* Adult Dataset
** methodology
*** data
We predict whether income exceeds $50K/yr based on census data, on the adult
dataset. The task is binary classification, and the sensitive attribute is the
gender of the person.

*** encoder
1 hidden layer neural network as encoder, mapping to a 2 dimensional latent
space for both the target and sensitive representation.

*** discriminator
The discriminators have two hidden layers, Each hidden layer contains 64 units

*** target prediction
logistic regression
*** hyperparameters
The learning rate for all components is 10−3 and weight decay is 5 × 10−4
**** VAE opti
**** Sensitive discriminator
#+BEGIN_SRC text
MLP(
  (net): Sequential(
    (0): Linear(in_features=2, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
  (nonlinear): Sigmoid()
)
#+END_SRC
**** Target discriminator
#+BEGIN_SRC text
MLP(
  (net): Sequential(
    (0): Linear(in_features=2, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
  (nonlinear): Sigmoid()
)
#+END_SRC
**** Target predictor
The target predictor is a =sklearn.linear_model.LogisticRegression= initialized
with default parameters.
**** Sensitive predictor

** scripts to reproduce
Within one epoch everything works fine already, so we can just run
#+BEGIN_SRC sh
python scripts/train.py --dataset adult --max_epochs 2 --z_dim 2 \
       --batch_size 64 --seed 420 --lambda_od 0.037 \
       --lambda_entropy 0.55 --gamma_od 0.8 --gamma_entropy 1.66
#+END_SRC


* German Dataset
** methodology
*** data
Binary classification for good/bad credit risk, the sensitive attribute is the
gender of the person.
*** encoder
1 hidden layer neural network as encoder, mapping to a 2 dimensional latent
space for both the target and sensitive representation.

*** discriminator

The discriminators have two hidden layers, with a logistic regression output
layer. Each hidden layer contains 64 units

*** target prediciton
logistic regression
*** hyperparameters

The learning rate for all components is 10−3 and weight decay is 5 × 10−4
| =lambda_od= |   |
| =gamma_od=  |   |
** scripts to reproduce
If we run it for 30 epochs, we get a 0.76 accuracy on the target embeddings and
a simple majority vote by the sensitive predictor, strongly suggesting there is
nothing to learn in the target embeddings to predict the sensitive attributes
# this is our own found solution with 0.76 acc
# #+BEGIN_SRC sh
# python scripts/train.py --dataset german --max_epochs 20 --z_dim 2 \
#         --seed 420 --lambda_entropy 0.45 --gamma_entropy 2 \
#        --lambda_od 0.15 --gamma_od 0.1
# #+END_SRC

According to their paper
#+BEGIN_SRC sh
python scripts/train.py --dataset german  --z_dim 2 \
        --seed 420 --lambda_entropy 1 --gamma_entropy 2 \
       --lambda_od 0.01 --gamma_od 1.4 --max_epochs 12
#+END_SRC

* Yaleb Dataset
*** data
The target task is the identification of the subject while the light source
condition is considered the sensitive attribute
*** encoder
 encoder consisted of one layer
*** discriminator
 and the discriminator is neural network
with two hidden layers each contains 100 units.
*** target prediction
target predictor is one linear layer
*** hyperparameters
 The parameters are trained using Adam optimizer with a
learning rate of 10−4 and weight decay of 5 × 10−2

*** from other paper
We use a one-layer neural network for the encoder and a one-layer neural network for prediction. γ is
set to 2. The discriminator is a two-layer neural network with batch normalization. The batch size is
set to 16 and the hidden size is set to 100.

** param sweep

these are the hyperparameters we keep constant:
- fodvae_learning_rate: 1e-4
- fodvae_weight_decay: 5 * (10 ** -2)
- z_dim: 100
- max_epochs: ??
(architectural hparams:)
# - encoder_input_dim: 32256
# - discriminator_target_output_dim: 38
# - discriminator_sens_output_dim: 5
- encoder_hidden_dims: []
- encoder_hidden_nonlinearity: ReLU
- discriminator_target_hidden_dims: [100, 100]
- discriminator_target_hidden_nonlinearity: ReLU
- discriminator_sens_hidden_dims: [100, 100]
- discriminator_sens_hidden_nonlinearity: ReLU
- batch_size: 16

these are the hyperparameters that we will vary (between ranges):
- lambda_od
- lambda_entropy
- gamma_od
- gamma_entropy
- step_size

right now, these hyperparameters are not implemented even:
- sensitive_predictor_learning_rate
- sensitive_predictor_weight_decay

** scripts to reproduce
#+BEGIN_SRC python
python scripts/train.py --dataset yaleb --lambda_od 0.037 --lambda_entropy 1 \
    --gamma_od 1.1 --gamma_entropy 2 --z_dim 100
#+END_SRC
* CIFAR
** methodology
*** data
The original dataset contains 10 classes we refer to as fine classes, we divide
the 10 classes into two categories living and non-living classes and refer to
this split as coarse classes.  The target task is the classification of the
coarse classes while not revealing information about the fine classes.

For cifar 100 a similar task except the coarse classes are as defined in
[[https://arxiv.org/pdf/1904.05514.pdf][this paper]] in table 1.
*** encoder
ResNet-18 [7] architecture for training the encoder
*** discriminator
a neural network with two hidden layers (256 and 128 neurons).
*** target predictor
a neural network with two hidden layers (256 and 128 neurons).
*** hyperparameters
For the encoder, we set the learning rate to 10−4 and weight decay to 10−2. For the
target and discriminator networks, the learning rate and weight decay were set
to 10−2 and 10−3
,respectively.
** scripts to reproduce
