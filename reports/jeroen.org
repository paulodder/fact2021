#+BIND: org-export-use-babel nil
#+TITLE: jeroen experiences during project
#+AUTHOR: jeroen jagt
#+EMAIL: <jpjagt@pm.me>
#+DATE: January 7, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session jeroen :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* data

** normalization

we assume the data is normalized because we get unworkable embeddings due to
large values in the input if we do not normalize the data. however, the type of
normalization is not specified in the paper. hence, we perform z-normalization
for all non-categorical columns in the tabular datasets (adult and german).

** yaleb

two versions of the extended yaleb dataset exist; original and cropped. in the
cropped version, images are aligned and cropped. we use the cropped version, as
it is easier to work with.

additionally, the lighting positions corresponding to the images used for the
training partition of the data are specified as "top left, top, top right,
bottom left, bottom right", but only coordinates are specified in the
dataset. hence, we estimated these positions to correspond to the coordinates

#+BEGIN_SRC python
train_positions = {
    (-110, 65),
    (0, 90),
    (110, 65),
    (110, -20),
    (-110, -20),
}
#+END_SRC

based on this plot of these coordinates present in the dataset:

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION:
#+label: fig:yaleb_lighting_positions
[[file:../plots/yaleb_lighting_positions.png]]

update: so we have inferred that these five positions count for all
coordinates, and that we should thus cluster all coordinates into five main
positions ("top left, top, top right, bottom left, bottom right"). we have done
so empirically, as no clusters were predefined.

one snippet of info that we can infer from the results plot is the majority
vote baseline in the sensitive attribute classification accuracy (Fig 2b),
which for the yaleb dataset sits around =0.51=. this suggests that one single
class (out of five) accounts for 51% of the test samples.

also, their test size is 1096; together with =38x5 = 190= train samples, that's
have a total size of 1286. however, our dataset contains a total of =2433=
samples (190 train, 2243 test). there are a total of 18 corrupted (".bad")
images in the set, so the difference can't have to do with that.

we can do some dataset arithmetic, however. there are:
- 18 "bad"/corrupted images in the set
- 38 persons
- 64 total positions

their test size was 1096. perhaps some lighting positions were omitted due to
ambiguity as to their cluster membership. each lighting positions contains 38
people. so, the number of lighting positions should be =1096 / 38 = 28.84=,
which is not a whole number, so it is not possibe. however, if we assume that
some number of corrupted images were discarded, the actual number of samples
could be anywhere between 1096+1 and 1096+18. dividing these candidates by 38,
we see that =(1096 + 6) / 38 = 29=, the only integer in the resulting
numbers. so it is possible that 29 lighting positions were selected for
testing, that those lighting positions contained 6 corrupted images, which were
discarded, and that the resulting test size was 1096.
if we count the number of bad images per lighting position, we get:

#+BEGIN_EXAMPLE
{('-010', '+00'): 3,
 ('+095', '+00'): 4,
 ('-050', '-40'): 3,
 ('+050', '-40'): 3,
 ('-110', '+15'): 3,
 ('-110', '-20'): 1,
 ('-035', '+40'): 1}
#+END_EXAMPLE

so it does seem to be possible that a subset of these were among the positions
selected for the test set, and that the bad images were discarded.

** cifar100

the authors, following [20], repurpose the cifar100 dataset to become a
privacy-sensitive dataset. cifar100 has 100 fine classes, which are categorized
in 20 coarse super-classes. the fine classes are sensitive attribute, coarse
classes target attribute.

however, it is not said whether or not the fine classes are included as input
to the model. even though it would make sense because sensitive attributes are
also included in other datasets, it would be non-trivial to shape the input in
a 2d rectangle, and thus would not make a lot of sense when using a CNN. hence,
we don't add them as input.

** german

for the german dataset, there is no predefined train/test split. the paper also
does not specify anything about this, so we have defined a split of our own,
by setting some fraction to be the train split size.
