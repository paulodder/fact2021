#+BIND: org-export-use-babel nil
#+TITLE: jeroen experiences during project
#+AUTHOR: jeroen jagt
#+EMAIL: <jpjagt@pm.me>
#+DATE: January 7, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session jeroen :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* data

** normalization

we assume the data is normalized because we get unworkable embeddings due to
large values in the input if we do not normalize the data. however, the type of
normalization is not specified in the paper. hence, we perform z-normalization
for all non-categorical columns in the tabular datasets (adult and german).

** yaleb

two versions of the extended yaleb dataset exist; original and cropped. in the
cropped version, images are aligned and cropped. we use the cropped version, as
it is easier to work with.

additionally, the lighting positions corresponding to the images used for the
training partition of the data are specified as "top left, top, top right,
bottom left, bottom right", but only coordinates are specified in the
dataset. hence, we estimated these positions to correspond to the coordinates

#+BEGIN_SRC python
train_positions = {
    (-110, 65),
    (0, 90),
    (110, 65),
    (110, -20),
    (-110, -20),
}
#+END_SRC

based on this plot of these coordinates present in the dataset:

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION:
#+label: fig:yaleb_lighting_positions
[[file:../plots/yaleb_lighting_positions.png]]

update: so we have inferred that these five positions count for all
coordinates, and that we should thus cluster all coordinates into five main
positions ("top left, top, top right, bottom left, bottom right"). we have done
so empirically, as no clusters were predefined.

one snippet of info that we can infer from the results plot is the majority
vote baseline in the sensitive attribute classification accuracy (Fig 2b),
which for the yaleb dataset sits around =0.51=. this suggests that one single
class (out of five) accounts for 51% of the test samples.

also, their test size is 1096; together with =38x5 = 190= train samples, that's
have a total size of 1286. however, our dataset contains a total of =2433=
samples (190 train, 2243 test). there are a total of 18 corrupted (".bad")
images in the set, so the difference can't have to do with that.

** cifar100

the authors, following [20], repurpose the cifar100 dataset to become a
privacy-sensitive dataset. cifar100 has 100 fine classes, which are categorized
in 20 coarse super-classes. the fine classes are sensitive attribute, coarse
classes target attribute.

however, it is not said whether or not the fine classes are included as input
to the model. even though it would make sense because sensitive attributes are
also included in other datasets, it would be non-trivial to shape the input in
a 2d rectangle, and thus would not make a lot of sense when using a CNN. hence,
we don't add them as input.

** german

for the german dataset, there is no predefined train/test split. the paper also
does not specify anything about this, so we have defined a split of our own,
by setting some fraction to be the train split size.
