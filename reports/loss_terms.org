#+BIND: org-export-use-babel nil
#+TITLE: Loss Terms
#+AUTHOR: Paul Lodder, Pim Meerdink
#+EMAIL: <paul_lodder@live.nl>
#+DATE: January 5, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \usepackage{tikz}
#+LATEX_HEADER_EXTRA: \usepackage{bm}
#+LATEX_HEADER_EXTRA: \usetikzlibrary{shapes,backgrounds}
#+LATEX_HEADER_EXTRA: \usepackage{verbatim}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session loss_terms :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex
* Introduction
This file serves the purpose of documenting our initial current understanding
of the loss terms presented in [[https://arxiv.org/pdf/2003.05707.pdf][our paper]]. We provide names for each of the loss
terms in *bold* as well, and introduce names for loss terms that have not been
explicitly named yet.
* Agents
Prior to introducing the loss terms, let us give a brief overview of the
structure of the framework and its agents. In essence we map all input to two
latent distributions which are meant to encode different aspects of the initial
features. In particular we aim to explicitly encode the sensitive information
separately from the target information, where the latter is used for downstream
tasks like prediction.\\
We have a *target encoder* and a *target discriminator*, parameterized
$\theta_{T}$ and $\phi_{T}$ respectively. The target encoder
maps our features to the target latent representation, which the discriminator
learns to map back to the corresponding label.  In addition, we have a
*sensitive encoder* and a *sensitive discriminator*, parameterized
$\theta_{s}$ and $\phi_{s}$ respectively. the sensitive encoder maps
input samples to latent samples such that the sensitive discriminator is able
to reconstruct the sensitive information from the latent representation.\\
The target encoder and sensitive encoder are parameterized by $\theta_{T}$ and
$\theta_{S}$, respectively. They share some parameters, $\theta = \theta_{T}
\cap \theta_{S}$. We define $\theta_{S}^{*} = \theta \setminus \theta_{T}$ and
$\theta_{T}^{*} = \theta \setminus \theta_{S}$

* Losses
The overall objective is defined as:
$$\underset{\theta_{T},\theta_{S},\phi{T},\phi{S}}{argmin}
\mathcal{L}_{T}(\theta_{T},\phi_{T}) +
\mathcal{L}_{S}(\theta_{S^{*}},\phi_{S}) \lambda_{E}\mathcal{L}_{E}(\theta_{T},
\phi_{S})  + \lambda_{OD}\mathcal{L}_{OD}(\phi_{T},\phi_{S})$$

| *Name*            | *Notation*                                 |
|-------------------+--------------------------------------------|
| /                 | <                                          |
| Target loss       | $\mathcal{L}_{T}(\theta_{T},\phi_{T})$     |
| Sensitive loss    | $\mathcal{L}_{S}(\theta_{S^{*}},\phi_{S})$ |
| Entropy loss      | $\mathcal{L}_{E}(\theta_{T}, \phi_{S})$    |
| OD loss           | $\mathcal{L}_{OD}(\phi_{T},\phi_{S})$      |
| Target OD loss    | $\mathcal{L}_{z_{T}}(\theta_{T})$          |
| Sensitive OD loss | $\mathcal{L}_{z_{S}}(\theta_{T})$            |

** *Target loss* ($\mathcal{L}_{T}(\theta_{T},\phi_{T})$)
$$\mathcal{L}_{T}(\theta_{T},\phi_{T}) = KL(p(y|x)\parallel q_{\phi_{t}}(y|z_{T}))$$
Important to note here is that this loss depends on the both target encoder
(generates target latent representation $z_{T}$ based on $x$) and discriminator
(responsible for $q_{\phi_{T}}(y|z_{T})$.\\
Intuitively, the target loss captures the extent to which the target discriminator,
parameterized by $\phi_{T}$ is able to correct mimic the true posterior
$p(y|x)$ based on the observed target latent variable $z_{T}$ as generated by
the target encoder.
** *Sensitive loss*
$$\mathcal{L}_{S}(\theta_{S}^{*},\phi_{S}) = KL(p(S|x)\parallel
q_{\phi_{S}}(y|z_{S}))$$\\
Highly analogous to the target loss, except it uses the sensitive
attributes. It depends on both the sensitive encoder (generates the sensitive
latent representation $z_{S}$ based on $x$) and the sensitive discriminator
(responsible for $z_{\phi_{S}}(y|z_{S})$).\\
Note that there is a small asymmetry with respect to the target loss, in that
this loss is only used to compute gradients with respect to $\theta^{*}_{S}$,
i.e. only those parameters that the sensitive encoder uses but that are not
used by the target encoder. We think this is done to prevent this intersection
of the parameters (i.e. $\theta$) to be updated twice as much.\\
The sensitive loss captures the extent to which our sensitive discriminator,
parameterized by $\phi_{S}$ is able to correct mimic the true posterior based
$p(s|x)$ on the observed target latent variable $z_{S}$ as generated by the
target encoder.
** *Entropy loss*
$$\mathcal{L}_{E}(\phi_{S},\theta_{T}) = KL(p(s|z_{T})\parallel\mathcal{U}(s))$$ This
is the entropy loss as it maximizes the entropy of the distribution over
sensitive classes $s$ given the target latent representation $z_{T}$. This term
maximizes the entropy as it forces the distribution $p(s|z_{T})$ to be as close
to the uniform distribution as possible. In essence this promotes fairness by
ensuring an equal distribution over sensitive classes for any given target
latent representation.
** *Orthogonal-disentangled loss*
$$\mathcal{L}_{OD}(\phi_{T},\phi_{S}) = \mathcal{L}_{z_{T}}(\theta_{T}) +
\mathcal{L}_{z_{S}}(\theta_{T})$$This term is called the orthogonal
disentangled loss (or OD loss) as it enforces disentanglement
(i.e. independence among dimensions) and orthogonality of the approximated
target $q_{\phi_{T}}(z_{T}|x)$ vs. sensitive $q_{\phi_{T}}(z_{S}|x)$ latent
distributions. Both of these conditions are enforced through the choice of the
priors.  The OD loss decomposes into the OD target and OD sensitive loss:
- $\mathcal{L}_{z_{T}}(\theta_{T}) = KL(q_{\theta_{T}}(z_{T} \vert x) \parallel
  p(z_{T}))$
- $\mathcal{L}_{z_{S}}(\theta_{S}) = KL(q_{\theta_{S}}(z_{S} \vert x) \parallel
  p(z_{S}))$
These terms enforce disentanglement by ensuring that the priors $p(z_{T})$ and
$p(z_{S})$ are decorrelated, i.e. $p(z_{T}) = \prod_{i=1}^{N_{T}}p(z_{T}^{i})$
and $p(z_{S}) = \prod_{i=1}^{N_{S}}p(z_{S}^{i})$. In particular, we can choose
a multivariate Gaussian with identity covariance, where $p(z_{T})$ and
$p(z_{S})$ are parameterized by $(\mu_{S}, \sigma_{S})$ and $(\mu_{M},
\sigma_{M})$, respectively.\\
Furthermore, the orthogonality constraint is enforced by choosing $\mu_{S}$
 and $\mu_{T}$ such that $\mu_{S} \perp \mu_{T}$
** *Worked out losses*
We can write out the OD target loss as follows,
#+BEGIN_EXPORT latex
\[
\begin{aligned}
  \mathcal{L}_{\bm{z}_{T}}(\theta_{T})
  &= KL(q_{\theta_{T}}(\bm{z}_{T} \vert \bm{x}) \parallel p(\bm{z}_{T})) \\
  &= \sum_{i=1}^{d_T} KL(q_{\theta_{T}}z_{T}^i \vert \bm{x}) \parallel p(z_{T}^i))


\end{aligned}
\]
#+END_EXPORT
because both the prior and the encoder posterior are independent Gaussian distributions, the
KL divergence between the two is simply a sum over KL divergences between the
univariate Gaussians $q_{\theta_{T}}(z_{T}^i \vert \bm{x})$ and $p(z_{T}^i)$.

One KL divergence terms can be computed as follows:
#+BEGIN_EXPORT latex
\[
  \begin{aligned}
    KL(q_{\theta_{T}}(z^i_T \vert \bm{x}) \parallel p(z^i_T))
    &= - \int q_{\theta_{T}}(z^i_T \vert \bm{x}) \log \frac{q_{\theta_{T}}(z^i_T \vert \bm{x})}
    {p(z^i_T)} d\bm{x} \\
    &= \frac{1}{2} \log (2 \pi \sigma_{p_T}^i)
    + \frac{(\sigma_{q_T}^i)^2(\mu_{q_T}^i - \mu_{p_T}^i)^2}{2 \sigma_{p_T}^i}
    - \frac{1}{2} (1 + \log 2\pi (\sigma_{q_T}^i)^2) \\
    &= \log \frac{\sigma_{p_T}^i}{\sigma_{q_T}^i}
    + \frac{(\sigma_{q_T}^i)^2(\mu_{q_T}^i - \mu_{p_T}^i)^2}{2 \sigma_{p_T}^i}
    - \frac{1}{2}
  \end{aligned}
\]
#+END_EXPORT
In practice, we will compute the element-wise KL divergence between the prior
and posterior and sum over the result. The OD sensitive loss can be computed in
a similar way.
