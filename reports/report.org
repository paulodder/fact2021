#+BIND: org-export-use-babel nil
#+TITLE: Report FACT 2021
#+AUTHOR: Jeroen Jagt, Paul Lodder, Pim Meerdink, Siem Teusink
#+EMAIL: <paul_lodder@live.nl>
#+DATE: January 8, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session report :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

#+BEGIN_SRC emacs-lisp :exports none
(setq org-export-with-toc nil)
(setq org-export-with-section-numbers nil)
(setq org-export-latex-hyperref-format "\\ref{%s}")
#+END_SRC

#+RESULTS:
: \ref{%s}


* Reproducibility Summary
Summary can be a page long max and needs to be included
** Scope of Reproducibility
** Methodology
** Results
** What was easy
** What was difficult
** Communication with original authors

\newpage
* 1 Introduction
- Shortly introduce the work at high level (few paragraphs AT MOST, guidelines
  talk about a couple of sentences).

The work of [cite Sarhan] introduces a new way of learning representations that
are useful for a specified task while robust against sensitive attributes. They
propose a method that decomposes the latent representation into a target and
sensitive representation. The representations are made to be orthogonal to
prevent sensitive information leaking into the target
representation. Furthermore, they introduce the disentanglement property to
"split the generative factors of each learned code" [cite Sarhan]. Last, they
ensure that the target representation is agnostic to the sensitive information
via maximum entropy.

* 2 Scope of reproducibility
- Introduce problems work addresses
- Introduce claims of paper that we will try to reproduce
[cite Sarhan] aimed to answer to questions with their experiments. 1) how does
the learned target representation perform when predicting target attributes,
and 2) how much information of the sensitive attributes is still present in the
learned target representation? To answer these questions they reported the a
/target accuracy/ and a /sensitive accuracy/ respectively, which we define in
section [[#sec:exp-setup]]. The authors' reported values can be found in
table [[tab:sarhan_accuracies]]. Note that some of the accuracies were not reported
exactly and have therefore been read of graphs. It is therefore our aim to
reproduce these accuracies found by the authors.


#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+attr_latex: :align c|c|c
#+CAPTION: Accuracies aquired by [cite Sarhan]. \textbf{Accuracies in bold} were not reported exactly and have been read of a figure.
#+label: tab:sarhan_accuracies
|-----------+-----------------+--------------------|
| dataset   | target accuracy | sensitive accuracy |
|-----------+-----------------+--------------------|
| Adult     | $\bm{0.68}$     | $0.6826$           |
| German    | $\bm{0.77}$     | $0.71$             |
| CIFAR-10  | $0.9725$        | $0.1907$           |
| CIFAR-100 | $0.7074$        | $0.1447$           |
| YaleB     | $\bm{0.92}$     | $\bm{0.52}$        |
|-----------+-----------------+--------------------|

* 3 Methodology
The code of the author's implementation of this project was not
available. Therefore, it was our goal to reproduce the implementation from the
description in the paper.
- Summarise resources used (GPU, other documentation (perhaps documentation for
  datasets?) or code)
The essential elements of the model are described in
the next section.
** 3.1 Model descriptions
- Give explanation of the model
  - Alterations for different data-sets
  - /Is this a good place to give details of all the losses?/
** 3.2 Datasets
- For every dataset list:
  - Number of examples and label distributions
  - Train/test split details
  - Pre-processing description
  - Link to download data (do not know if this is really something we want to
    do here, we can make sure the reference includes the link)
** 3.3 Hyperparameters
- Describe process of setting hyperparameters
- If hyperparameter search was performed, denote the details (amount of
  searches, what space, etc.)
** 3.4 Experimental setup and code
:PROPERTIES:
:CUSTOM_ID: sec:exp-setup
:END:
- Setup description that allows for replication
- Explain evaluation metric
** 3.5 Computational Requirements
- Include Hardware used (CPU/GPU used)
- For each model, include average run-time
- Include /total hours of GPU time/

* 4 Results
- High-level overview. Do our results support the paper's claims? Keep
  discussion for next section.
** 4.1 Results reproducing original paper
- Report results and make clear which claim they support (if they do). Should
  be done in logical sections.
** 4.2 Results beyond original paper
- Report additional results we acquired, if relevant.

* 5 Discussion
- Discuss whether we think our results support the claims of the paper. Discuss
  strengths and/or weaknesses of our approach.
** 5.1 What was easy
** 5.2 What was difficult
** 5.3 Communication with original authors
* References
