#+BIND: org-export-use-babel nil
#+TITLE: Report FACT 2021
#+AUTHOR: Jeroen Jagt, Paul Lodder, Pim Meerdink, Siem Teusink
#+EMAIL: <paul_lodder@live.nl>
#+DATE: January 8, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage[]{neurips_2019}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc} % allow utf-8 input
#+LaTeX_HEADER: \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
#+LaTeX_HEADER: \usepackage{hyperref}       % hyperlinks
#+LaTeX_HEADER: \usepackage{url}            % simple URL typesetting
#+LaTeX_HEADER: \usepackage{booktabs}       % professional-quality tables
#+LaTeX_HEADER: \usepackage{amsfonts}       % blackboard math symbols
#+LaTeX_HEADER: \usepackage{nicefrac}       % compact symbols for 1/2, etc.
#+LaTeX_HEADER: \usepackage{microtype}      % microtypography
#+LaTeX_HEADER: \usepackage{multirow}
#+LaTeX_HEADER: \usepackage{subcaption}
#+LaTeX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LaTeX_HEADER: \usepackage[normalem]{ulem}
#+LaTeX_HEADER: \newif{\ifhidecomments}
# #+LaTeX_HEADER: \usepackage{minted}
# #+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
# #+LATEX_HEADER: \usepackage{bm}
# #+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
# #+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
# #+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session report :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex
#+BIBLIOGRAPHY: refs plain

* settings :noexport:
#+BEGIN_SRC emacs-lisp :exports none
(setq org-export-with-toc nil)
(setq org-export-with-section-numbers nil)
;; (setq org-export-latex-hyperref-format "\\ref{%s}")

(package-initialize)
(use-package ox-latex-subfigure
  :init
  (setq org-latex-prefer-user-labels t)
  :load-path "~/Dropbox/ProjectWeekends/lisp/ox-latex-subfigure/"
  :config (require 'ox-latex-subfigure))

(require 'org-ref)
(setq org-ref-default-bibliography "refs.bib")
;; (setq org-latex-pdf-process (list "latexmk -pdf %f -shell-escape"))

#+END_SRC

#+RESULTS:
: refs.bib

* Reproducibility Summary
Summary can be a page long max and needs to be included
** Scope of Reproducibility
** Methodology
** Results
** What was easy
** What was difficult
** Communication with original authors

\newpage
* 1 Introduction

The work of \cite{sarhan2020fairness} proposes a new way to learn
representations that, while meaningful for some specified target task, do not
contain any information about a sensitive attribute present in the original
sample. The proposed method to achieve this invariability towards the sensitive
attribute is to disentangle the latent representation into independent target
and sensitive representation. As a proxy for independence, orthogonality is
enforced between these individual representations. Furthermore, in order to
prevent sensitive information leaking into the target representation, the model
is trained to learn a target representation which is agnostic to the sensitive
information, using a maximum entropy approach.

In order to consolidate the claims brought forth in \cite{sarhan2020fairness},
and assess the reproducibility of this work, our research attempts to reproduce
the achieved results by reimplementing the proposed method. The remainder of
this report is as follows. In the next section, we specify the parts of the original work
that we attempt to reproduce. In Section 3, we summarize the method as proposed in
\cite{sarhan2020fairness} that we attempt to reimplement. In Section 4, we
discuss the results we attain. Finally, we discuss the results, and we analyse
the reproducibility of the reference work.

* 2 Scope of reproducibility

# jpj: <<TODO>> is this paragraph needed?
The reference work by Sarhan et al. works towards a method of generating
embeddings of data, while remaining invariant towards a particular
feature. This is done by explicitly learning an encoding of the sensitive
information, and forcing this encoding to be orthogonal and disentangled with
respect to the actual `target' embeddings that are to be used in the downstream
task.

The eficacy of the proposed method is assessed using two evaluative
questions. First, how well can the learned target representation be used in the
target task? Second, to what degree is information which might reveal the
sensitive attribute still present in the learned target representation?

In the reference work, a collection of three single- and two multi-class
classification tasks are considered for a total of five classification tasks,
corresponding to five different datasets. For each of these tasks, a version of
the proposed model is trained, and evaluated using two metrics: /target
accuracy/ and a /sensitive accuracy/, which correspond to the two
aforementioned questions. Both of these accuracies, for each of the five tasks
at hand, are included in our reproduction.

Furthermore, an ablative study is conducted in the reference work, in which
specific components of the loss function used to train the model are excluded
(i.e., ablated), in order to observe the behaviour of the model, and understand
the role of each of these loss components within the training process. For
instance, the orthogonality constraint of the learned sensitive and target
embeddings can be ignored by omitting the relevant loss
component(s). This ablative study, which entails the evaluation of the impact
of six unique combinations of loss components, is performed on each of the five datasets,
and is included in our reproduction.

# jpj: could be more clear, this paragraph
Finally, the authors perform a sensitivity analysis on the hyperparameters that
weigh the different loss terms. In particular, the final target accuracy and
sensitivity accuracy are reported as a function of the weight assigned to the
entropy loss component, and the KL loss component (c.f. Section 3). These
results are displayed as a heatmap. In similar fashion, an additional heatmap
is constructed for two hyperparameters that control the decay of these loss terms.

* 3 Methodology

As the code of the original implementation was not available, it was our goal
to reproduce the method, based on all implementation details expounded in the
reference work. The essential elements of the model are described in the next
section.

** 3.1 Model descriptions

Let $\mathcal{X}$ be the dataset and let $\bm{x} \in \mathbb{R}^D$ be a single
input sample. Each sample has an associated target vector $\bm{y} \in
\mathbb{R}^n$ and an associated sensitive attribute vector $\bm{s} \in
\mathbb{R}^m$, with $n$ and $m$ classes respectively. The aim is to create two
latent representations; a target latent representation $\bm{z}_T$ and a
sensitive latent representation $\bm{z}_S$. The encoder has the following shape:
The first part of the encoder $f(\bm{x}, \theta)$ can be shared between the target
and sensitive representation, depending on the dataset. After a data sample
$\bm{x}$ is fed through said shared encoder, the result is fed through two separate encoders,
$q_{\theta_T}(\bm{z}_T | \bm{x})$ and $q_{\theta_S}(\bm{z}_S | \bm{x})$, to
create the target and sensitive representations respectively. The target and
sensitive encoders are parameterized by $\theta_T$ and $\theta_S$
respectively. The shared part of the encoder is therefore paramaterized by
$\theta = \theta_T \cap \theta_S$.
Each representation is then used as input for the corresponding discriminator,
either the target discriminator $q_{\phi_T}(\bm{z}_T | \bm{x})$ or the
sensitive discriminator $q_{\theta_S}(\bm{z}_S | \bm{x})$.
The encoder and discriminator are trained in supervised fashion to minimize the
following losses, which combined we call the representation loss:
\begin{align}
\label{eq:recon-losses}
\mathcal{L}_{T}(\theta_{T},\phi_{T}) &= KL(p(\bm{y}|\bm{x})\parallel
q_{\phi_{t}}(\bm{y}|\bm{z}_{T})) \\
\mathcal{L}_{S}(\theta_{S}^{*},\phi_{S}) &= KL(p(\bm{s}|\bm{x})\parallel
q_{\phi_{S}}(\bm{y}|\bm{z}_{S}))
\end{align}

Here $\theta_S^* = \theta_S \backslash \theta$. These losses are effectively
equal to the cross-entropy between the predicted values for the targets and
sensitive attributes and their actual values.

To ensure no sensitive info leaking into the target representation, we
maximimze "the entropy of the sensitive discriminator given the target
representation". This is achieved by minimising
\begin{equation}
\label{eq:entropy-loss}
\mathcal{L}_{E}(\phi_{S},\theta_{T}) =
KL(q_{\phi_S}(\bm{s}|\bm{z}_{T})\parallel\mathcal{U}(\bm{s}))
\end{equation}

Last, we want to ensure that there is some level of independence between the
two representations, i.e., ideally the posterior $p(\bm{z}_T | \bm{x})$ would
be statistically independent of $p(\bm{z}_S | \bm{x})$. Following
\cite{sarhan2020fairness} we relax this independence requirement to
enforcing 1) a disentaglement property and 2) orthogonality between the two
representations. To enforce these properties, we need to /estimate/ the
aforementioned posteriors (as they are intractable) using Variational
Inference. The encoder network will be similar to the encoder of a Variational
Auto-Encoder (VAE) model [cite VAE], in that it outputs the means $(\bm{\mu}_T,
\bm{\mu}_S)$ and variances $(\bm{\sigma}_T, \bm{\sigma}_S)$ for both
representations. To enforce the disentanglement property, we minimize the
KL-divergence between the output posterior $q_{\theta_T} (\bm{z}_T | \bm{x})$
and some prior $p (\bm{z}_T)$:
# jpj: is this indeed the 'disentanglement' property?

\begin{align}
\label{eq:od-losses}
\mathcal{L}_{z_{T}}(\theta_{T}) &= KL(q_{\theta_{T}}(\bm{z}_{T} \vert \bm{x}) \parallel
  p(\bm{z}_{T}))
\end{align}

Here $q_{\theta_T} (\bm{z}_T | \bm{x}) = \mathcal{N} (\bm{z}_T | \bm{\mu}_T,
\text{diag} (\bm{\sigma}_T ^2))$ and $p (\bm{z}_T) = \mathcal{N} (\bm{0} ,
\bm{I})$. We can construct a similar KL-divergence term for the sensitive
representations. To enforce the orthogonality between the two representations
we can make sure that the means of the prior distributions are orthogonal. This
will indirectly push the posterior distributions to be orthogonal.

We combine these two loss terms into a single term, which we call the /Orthogonal
Disentangled/, or /OD/ loss.

$$
\mathcal{L}_{OD}(\theta_{T}, \theta_S) = \mathcal{L}_{z_{T}}(\theta_{T})  +
\mathcal{L}_{z_{S}}(\theta_{S})
$$

We can use the re-parameterization trick [cite reparam trick] to sample
from the posterior distribution to obtain the latent representations, which can
then be fed to the respective discriminators.

We combine all of the aforementioned losses and the objective becomes to
minimize this combined loss:
\begin{equation}
\label{eq:total-loss}
\underset{\theta_{T},\theta_{S},\phi{T},\phi{S}}{argmin}
\mathcal{L}_{T}(\theta_{T},\phi_{T}) +
\mathcal{L}_{S}(\theta_{S^{*}},\phi_{S}) \lambda_{E}\mathcal{L}_{E}(\theta_{T},
\phi_{S})  + \lambda_{OD}\mathcal{L}_{OD}(\phi_{T},\phi_{S})
\end{equation}

Here $\lambda_{OD}$ and $\lambda_E$ are weights of the OD loss and the maximum
entropy loss respectively. Additionally, we introduce two decay parameters,
$\gamma_{OD}$ and $\gamma_{E}$ which allows us to change the weights of the
aforementioned losses while training. The OD loss weight at epoch $t$ during
training will be calculated as follows:
\begin{equation}
\lambda_{OD}^{(t)} = \lambda_{OD}^{(0)} \gamma_{OD}^{t/t_s}
\end{equation}
Here $t_s$ is the so-called /step-size/ parameter, and $\lambda_{OD} is the
initial OD loss weight. The maximum entropy loss weight will be computed in the
same way. $\lambda_{OD}^{(0)}, \lambda_{E}^{(0)}, \gamma_{OD},
\gamma_{E}$ and $t_s$ are all hyperparamters that we need to set. More
information on those will follow.

** 3.2 Datasets
In order to reproduce the results obtained by Sarhan et al. it was necessary to
apply the model to five datasets. Below, we outline some basic properties of
the datasets, as well as outlining steps taken in the pre processing, and we
explain the sensitive and target attributes that are to be modelled.

*** Tabular data
The Adult and German dataset were obtained from the UCI repository cite:uci.
Both of these datasets contain census data, and include categorical and
continuous attributes which contain information about the person's gender,
education, and occupation. For both datasets, preprocessing consisted of
representing categorical columns in a one-hot encoding, where missing values
were explicitly encoded as a separate category, while continuous variables were
left unchanged.

For the Adult dataset, the task is to predict whether a persons income exceeds
$\$50,000$. In total, it contains 48842 rows and 14 attributes. After
preprocessing, each input sample consisted of 108 features. The sensitive
attribute for this dataset is binary gender. Approximately 75\% of the samples'
incomes do not exceed $\$50,000$, and around 67\% are male. The train/test
split is explicitly defined in the repository, and the train:test ratio is 2:1.

The German dataset contains 1000 rows with 20 attributes, and the task is to
classify rows as having good or bad credit risk. Similar to the Adult dataset,
the sensitive attribute is gender. After preprocessing, each input sample
consisted of 61 features. As no train/test split was defined in the reference
work nor in the dataset itself, we chose to split the data with a train:test
ratio of 4:1. 68\% of the rows in the dataset are male, and 70\% have bad
credit risk.

*** YaleB data

# jpj: we should reference the original papers that Sarhan et al. reference
# as well, in corresponding places.
The Extended YaleB dataset was collected from the University of Toronto
computer science department website cite:GeBeKr01. The `Cropped' version of the
dataset was used \citep{KCLee05}, which contains 2433 grayscale, $192 \times
168$ images of 38 human faces under different lighting conditions. The task is
to identify to which of the 38 humans an image corresponds. We constructed a
sensitive attribute by clustering the illumination conditions into 5 classes
loosely corresponding to top left, bottom left, top right, bottom right and
center. We defined these classes ourselves as we were unable to find detailed
information on how this was done in the study by Sarhan et al. Our sensitive
attributes' distribution was not skewed, with the `front' class containing
around 800 images, and the between 340 and 380. This was not in line with the
paper by Sarhan et al, who mention that a majority class classifier could
attain 50\% accuracy, in our case this is around 0.35. Unfortunately, we were
unable to find sufficient information to be able to replicate the ratios
mentioned in the reference paper, and instead constructed our own sensitive
attributes.

Our training dataset comprised of 190 images, like in the reference work. It is
important to note that our testing dataset contained 2243 images, while the
original study's dataset contained only 1096. The reason for this is unclear,
as we used the full dataset, and found no mention of the omission of certain
images in the reference paper. The images were flattened into vectors
of length $32,256$. The target feature was evenly distributed across the dataset,
i.e. the dataset contained 64 images of each person.

*** CIFAR data

The CIFAR-10 and CIFAR-100 datasets were also collected from the University of
Toronto computer science department website cite:GeBeKr01. For both datasets,
the predefined train:test ratio was 5:1, and no pre-processing was applied
beyond the construction of target labels.

CIFAR-10 consists of $60,000$ $32 \times 32 \times 3$ colour images that are
divided into 10 classes such as airplane, automobile and bird. For our
purposes, we construct a new target attribute, one that denotes whether the
subject of the image is alive or not. The sensitive attribute, then, is the
original label of the image. There are $6,000$ images of each original class
label. Of all images, 60\% are labelled as alive, and 40\% are not.

The CIFAR-100 dataset is similar to CIFAR-10, except that images are
categorized as one of 100 total fine-grained classes. These 100 fine classes
are split into 20 coarse classes that cluster similar concepts into one
category. For example: `beaver', `dolphin' and `otter' all belong to the coarse
class `aquatic mammals' (c.f.  cite:proteek). Here, the coarse class of an
image is used as the target attribute, while its fine class is used as the
sensitive attribute. There are 600 images of each fine class, and 3000 images
of each coarse class.

** 3.3 Implementation details

Following the paper of cite:sarhan2020fairness, we implement the following
networks for the several datasets. Note that, for every MLP mentioned below,
ReLU's are used as (non-final) activation functions.

#+BEGIN_EXPORT latex
\begin{table}[h!]
  \begin{center}
    \caption{Encoder and discriminator implementation details.}
    \label{tab:imp}
    \begin{tabular}{l|c|c|c|c|c}
      \hline
      \multirow{2}{*}{} & \multicolumn{3}{c|}{Encoder} & \multicolumn{2}{c}{Discriminator} \\
      \hline
      & Network Type & Hidden Dims &  Latent Dim & Network type & Hidden Dims  \\
      \hline
      Tabular & MLP       & 64  & 2   & MLP & 64, 64 \\
      \hline
      YaleB   & MLP       & 100 & 100 & MLP & 100, 100 \\
      \hline
      CIFAR   & ResNet-18 & -   & 128 & MLP & 256, 128 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
#+END_EXPORT

** 3.4 Hyperparameters
Most used hyperparameters were taken directly from the supplement provided by
Sarhan et al. Optimal values for some hyperparameters were not reported,
and as a result we empirically set these to values that seemed to result in
satisfactory performance. We discuss which hyperparameters we were missing in
the discussion section, and report all hyperparameters that we used in the
Appendix.
** 3.5 Experimental setup and code
:PROPERTIES:
:CUSTOM_ID: sec:exp-setup
:END:
*** Setup Reproducibility
Our implementation and instructions to run the code are available at
[[https://github.com/paulodder/fact2021]] (to be cleaned up). The repository
contains a folder =scripts= that contains all the scripts necessary to
perform several tasks. All dataset can be downloaded using the
=get_all.sh= script. The model can be trained and evaluated using =train.py=,
the ablative study can be ran using =ablative.sh= and the sensitive analysis
can be performed using =sensitive_analysis.py=. If necessary, =make_fig2.py=
can be used to make figures, such as \ref{fig:adult_german_yaleb}, of the results.
*** Evaluation
Evaluation of the embeddings learned by our model is non trivial, as we must
gather whether the embeddings adequately represent the data for the downstream
task (e.g. classification), while also ensuring that the embeddings contain no
sensitive information. In order to quantitively evaluate our model after
completing training, we train two classifiers. These classifiers use the test
data that is embedded using our trained model in the target space.

The first classifier, known as the /target predictor/ is trained to predict the
target label from the target embeddings. In accordance with the reference
paper, we evaluated the target predictor using accuracy as metric. It is
desirable that the target predictor performs as well as possible, as this means
that the target embeddings embed the information necessary for the downstream
task well.

The second classifier, known as the /sensitive predictor/ is trained to predict
the sensitive attribute from the target label. It is desirable that this
classifier preforms poorly, as we would like there to be no information
pertaining to the sensitive attribute in our target embedding. As such, we
would like the model to be as close to a 'majority classifier' as possible,
where the model is forced to simply predict the majority label for each data
row as it has no meaningful information with which to make a prediction about
the sensitive attribute. Again, we use solely accuracy as evaluation metric.

** 3.6 Computational Requirements

#+NAME: tab:computational_details
#+ATTR_LATEX: :caption The average run-time for each of the five datasets and their configurations.
| Dataset                 | Adult | German | YaleB | CIFAR-10 | CIFAR-100 | Total |
| Average run-time (min.) |   0.8 |   0.22 |     2 |       11 |        19 |    62 |
| Number of epochs        |     2 |     15 |    30 |       30 |        55 |     - |
# <<TODO>> fix this ugly ass org-table @paul

We used Google Colab Pro to train our models, which supplies one =Tesla
V100-SXM2-16GB= GPU, and 2 =Intel(R) Xeon(R) CPU @ 2.00GHz= CPUs. Average
run-times are specified in Table [[tab:computational_details]]. In order to train
all models over various seeds for all results, this would be the estimated
required run-time:
$$
(3 * 62) + (5 * 5 * 62) + (8^2 * 5 * 0.8) = 4,296 \text{ minutes}
$$
# To generate results, we need this # of runs:
# - normal: 3 * (all datasets)
# - ablative: 5 * 5 * (all datasets)
# - sensitivity: (8 ** 2) * 5 * (adult)

* 4 Results

In order to judge the reproducibility of the model proposed by
\cite{sarhan2020fairness}, we compare their results with those results we were
able to attain using our implementation. First, we compare target and sensitive
accuracy attained by training and evaluating the proposed model on each of the
five datasets. Second, we compare the ablative study with the ablative study of
Sarhan et al. Finally, we make the same comparison for the sensitive study.

** 4.1 Results reproducing original paper

*** CIFAR-10 and CIFAR-100

#+BEGIN_EXPORT latex
\begin{table}[h!]
  \begin{center}
    \caption{Results on CIFAR-10 and CIFAR-100 datasets}
    \label{tab:table1}
    \begin{tabular}{l|c|c|c|c}
      \hline
      \multirow{2}{*}{} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\
      \hline
      & Target Acc. \uparrow & Sensitive Acc. \downarrow & Target Acc. \uparrow & Sensitive Acc. \downarrow \\
      \hline
      Sarhan et al. & 0.9725 & 0.1907 & 0.7074 & 0.1447 \\
      Ours & 0.9582 & 0.3462 & 0.0500 & 0.0100 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

While we have been able to reproduce the CIFAR-10 target accuracy attained by
Sarhan et al., the CIFAR-10 sensitive accuracy we attained is
substantially higher than theirs, as displayed in Table \ref{tab:table1}. As
for the CIFAR-100 dataset, our results strongly differed from those reported by Sarhan et al.,
as our model was not able to learn a representation that carried meaningful
information, resulting in target and sensitive accuracies that are equal to
accuracies attained by majority vote (see Table \ref{tab:table1}).
#+END_EXPORT

*** Adult, YaleB, and German

#+BEGIN_EXPORT latex
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/adult_target.png}
         \caption{Adult target accuracy}
         \label{fig:adult_target}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/german_target.png}
         \caption{German target accuracy}
         \label{fig:german_target}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/yaleb_target.png}
         \caption{YaleB target accuracy}
         \label{fig:yaleb_target}
     \end{subfigure}

     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/adult_sens.png}
         \caption{Adult sensitive accuracy}
         \label{fig:adult_sens}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/german_sens.png}
         \caption{German sensitive accuracy}
         \label{fig:german_sens}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/yaleb_sens.png}
         \caption{YaleB sensitive accuracy}
         \label{fig:yaleb_sens}
     \end{subfigure}

     \caption{Performance of the proposed model, together
     with majority label classifier (denoted by the horizontal dashed line) and various
     other models for Adult, German, and YaleB datasets, compared between Sarhan et al. and our reproduction. The bars denoted by X correspond to direct use of the
input data for our target prediction. Furthermore, a VAE was trained on the
Adult and German datasets using MSE loss as reconstruction loss, and the
accuracies denoted with `VAE' correspond to the performance achieved by target
and sensitive predictors trained on these VAE embeddings as input features. For
YaleB, Logistic Regression was also performed on the raw data to predict the
sensitive and target attributes, whose performance is denoted by `LR'.}
     \label{fig:adult_german_yaleb}
\end{figure}

Note that for the following results, we focus on the comparison between performances of the proposed models. We have included a comparison of the alternative models in Figure \ref{fig:adult_german_yaleb} mainly to be able to investigate discrepancies in our reimplementation outside of the proposed method itself (e.g. significant differences in the dataset definition, pre-processing, et cetera).

Our results for Adult, as displayed in Figure \ref{fig:adult_german_yaleb}, are
similar to those obtained by \cite{sarhan2020fairness}, with the only
difference being a small increase in our sensitive accuracy with regards to
theirs.
As for German, we observe similar, yet not identical, target and
sensitive accuracies. We have to note that for runs during training with
certain random seeds, a target accuracy was obtained that was identical to the $76\%$
reported by Sarhan et al.; however, over multiple runs, we obtain a lower
accuracy around $73\%$ (see Figure \ref{fig:adult_german_yaleb}).
For YaleB, we were not able to reproduce the accuracies reported by Sarhan et al. Instead,
our model achieved a lower target accuracy, and a sensitive accuracy which is further away from the majority label classifier, suggesting that our model's
performance was worse than that of Sarhan et al.
#+END_EXPORT

*** Ablative
#+BEGIN_SRC sh
# # adult
# bash scripts/ablative.sh adult
# python scripts/visualize_ablative.py -d adult
# # german
# bash scripts/ablative.sh german
# python scripts/visualize_ablative.py -d german
# # yaleb
# bash scripts/ablative.sh yaleb
# python scripts/visualize_ablative.py -d yaleb
# # cifar10
# bash scripts/ablative.sh cifar10
# python scripts/visualize_ablative.py -d cifar10
# # cifar100
# bash scripts/ablative.sh cifar100
# python scripts/visualize_ablative.py -d cifar100
#+END_SRC

#+BEGIN_EXPORT latex
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/ablative.german.png}
         \caption{German}
         \label{fig:ablative_german}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/ablative.adult.png}
         \caption{Adult}
         \label{fig:ablative_adult}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/ablative.cifar10.png}
         \caption{CIFAR-10}
         \label{fig:ablative_cifar10}
     \end{subfigure}

     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/ablative.cifar100.png}
         \caption{CIFAR-100}
         \label{fig:ablative_cifar100}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../figures/ablative.yaleb.png}
         \caption{YaleB}
         \label{fig:ablative_yaleb}
     \end{subfigure}

     \caption{Target and sensitive accuracies of our model trained using various combinations of loss term components, results are averaged over 5 random seeds. Specifically, Entropy refers to the $\mathcal{L}_E$ component, Orth refers to the orthogonality constraint between the prior means, and KL refers to the $\mathcal{L}_{OD}$ component (c.f. \cite{sarhan2020fairness}).}
     \label{fig:ablative}
\end{figure}

The results of our ablative study are shown in Figure \ref{fig:ablative}, which can be compared with the ablative study of Sarhan et al. in Figure \ref{fig:sarhan_ablative} in Appendix B.
The baseline measurement was omitted as it was unclear from the text what it
entailed. In comparison to Sarhan et al., for German, we see that varying loss components seems to have less impact on performance; for Adult, we see similar invariability for target accuracy but a lower impact on sensitive accuracy; for CIFAR-10, we observe a larger variance in performance over seeds and loss components; and lastly, CIFAR-100 and YaleB results are significantly different.
In summary, our ablative study results generally do not exhibit the same patterns as those of Sarhan et al.

Apart from these comparisons, we observe that the `KL' loss component has two noticable and related effects: for CIFAR-10 and YaleB, the inclusion of this component entails a higher error bar (i.e., a higher variance in performance over multiple runs using different seeds), while for CIFAR-100, when this component is excluded, performance substantially increases beyond the range of majority vote.
#+END_EXPORT

*** Sensitivity

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Target and sensitive accuracies when varying $\lambda_{OD}$ together with $\lambda_E$ (left), and when varying $\gamma_{OD}$ together with $\gamma_E$ (right).
#+label: fig:sensitive_adult
[[file:../figures/sensitive.adult.png]]

The results of our sensitivity study are shown in Figure
\ref{fig:sensitive_adult}, which can be compared with the sensitivity study of
Sarhan et al. in Figure \ref{fig:sarhan_sensitive_adult} in Appendix B.

<<TODO>>

* 5 Discussion
/Discuss whether we think our results support the claims of the paper. Discuss
strengths and/or weaknesses of our approach./

The claim of the original authors are as follows: by disentangling the latent
representation of a data sample into two subspaces that are orthogonal to each
other, as well as training the model using a loss function that encourages it
to encode sensitive information into one of these subspaces, and meaningful
information for the task at hand into the other of those subspaces, it is
possible to create meaningful representations that do not contain any
information from which a protected, or sensitive, attribute can be inferred.

In order for our results to support this claim, they would need to show that
the proposed model is able to create representations that perform well on the
target task (i.e. attains a high target accuracy), while it performs poorly in
the inference of the sensitive attribute using the target representation
(i.e. attains a sensitive accuracy close to the accuracy of majority
voting). When looking at our results, we observe that this is indeed the case for the
German dataset. However, for the Adult and CIFAR-10 datasets, the attained
sensitive accuracy is substantially higher than the majority vote baseline; and
for the CIFAR-100 and YaleB datasets, the model does not achieve a satisfactory
performance in terms of target accuracy; and so, results from these four
datasets do not appear to support the original claim of the authors. Likewise,
those patterns that the authors observe in their ablative studies are not found
back in our own ablative studies.

This means that there is a discrepancy between our results and the original
results from \cite{sarhan2020fairness}. Thus, when considering the large effort
undertaken in this research to minutely reimplement their proposed method, we
conclude that the original paper is relatively difficult to reproduce, and can
in fact not be reproduced based solely on its contents.


** 5.1 What was easy
We experienced especially the theoretical part of the paper to be well
structured and though out. The set-up of the two types of
representations and notions of disentaglement and orthogonality makes sense
intuitively. Additionally, all loss terms are well described and were therefore
easy to implement.

** 5.2 What was difficult
\paragraph{Performance fluctuations and training instability}
One of the issues we ran into is that for these models training seems to be unstable,
which is evident from the high fluctuation in performance when we vary the
random seed or the  number of maximum epochs. (Reflected in high variance in
results also?) This is not addressed in the paper and therefore there is no
information on how to deal with it. To add to this, it was unclear what
trade-off between target and sensitive accuracy was used by the authors to
select the best model during training. This trade-off ultimately determines
which model is selected for testing which can have a large influence on
performance.

# I don't really now what to do with this part, seemed to be a substantial
# paragraph before but not if we leave those two commented points out perhaps
\paragraph{Implementation}
- Little information about given baseline models such as the VAE.

# These two were later resolved so maybe not mention them in the report?
# - Limited information on how certain losses were backpropagated with a shared
#   encoder network. (resolved after contact with authors)
# - Exact implementation of the $\lambda$ decay was not clearly reported
#  (resolved after contact with authors)


\paragraph{Hyperparameters}
The amount of epochs that the model was trained was not reported
in either the paper or its supplementary material. This was quite an important
value given that no explicit stopping criterion was mentioned, either. In
correspondene with Sarhan, we were able to set values for the step_size
hyperparameter that correspond to those used by the original team. Furthermore,
amongst the not reported hyperparameters were those involved the training of
the MLP target and sensitive predictors. These include the optimizer used, the
learning rate, weight decay, amount of epochs aswell as the nonlinearities, to name a
few.

\paragraph{Dataset details}
As mentioned in YaleB paragraph of the Datasets section we have made a number
of assumptions about how to set up the classes corresponding to the sensitive
attributes, which might have some influence on the performance of our approach
for this datasets.
We were unsure about some other details concerning the data as well. Namely,
the type of data-normalization is not specified, and for the German dataset there
is not a train-test split reported. However, these details were not as vital
for reproduction as the aforementioned YaleB issue.

** 5.3 Communication with original authors
We have had the pleasure of communicating with the original authors of the
paper. This helped getting our hands on some additional hyperparameters, such
as the stepsize $t_s$ and the dimensions of the latent representations for some
datasets, to name a few. Furthermore, we got insight in some implementation
details, such as how the loss weights $\labmda_{OS}$ and $\lambda_E$ are
updated and how the losses are backpropagated when dealing with a shared
encoder network. The authors were going to give us extra information on the
YaleB dataset specifically, but we were not able to receive said information in
time.

* Appendix
** A Hyperparameters
The hyperparameters that we used for our reported results can be found in table
\ref{tab:hps1} and \ref{tab:hps2}. Note that for all experiments we used the
Adam optimzer [cite Adam].

- We should check whether these are in fact the last hyperparameters we used
#+BEGIN_EXPORT latex
\begin{table}[h!]
  \begin{center}
    \caption{Hyperparameters that we used in our experiments for the various datasets.
    For the CIFAR datasets, the first number of the learning rate and weight decays refers
    to the encoder network and the second to the discriminator network.}
    \label{tab:hps1}
    \begin{tabular}{l|c|c|c|c}

      \hline
      & Learning Rate & Weight Decay & Batch Size & Max. Epochs  \\
      \hline
      Adult & $10^{-3}$ & $5 \times 10^{-4}$  & 64   & 2 \\
      \hline
      German & $10^{-3}$ & $5 \times 10^{-4}$  & 64   & 15 \\
      \hline
      YaleB   & $10^{-4}$ & $5 \times 10^{-2}$ & 64 & 30 \\
      \hline
      CIFAR-10   & $10^{-4}, 10^{-2}$ & $10^{-2}, 10^{-3}$ & 128 & 30 \\
      \hline
      CIFAR-100  & $10^{-4}, 10^{-2}$ & $10^{-2}, 10^{-3}$ & 128 & 80 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{table}[h!]
  \begin{center}
    \caption{The $\lambda_{OD}, \lambda_E, \gamma_{OD}$ and $\gamma_E$ we used in our experiments}
    \label{tab:hps2}
    \begin{tabular}{l|c|c|c|c}
      \hline
      & $\lambda_{OD}$ & $\lambda_E$ & $\gamma_{OD}$ & $\gamma_E$  \\
      \hline
      Adult  & $0.037$ & $0.55$  & $0.8$ & $1.66$ \\
      \hline
      German  & $0.01$ & $1.0$  & $1.4$ & $2.0$ \\
      \hline
      YaleB  & $0.037$ & $1.0$  & $1.1$ & $2.0$ \\
      \hline
      CIFAR-10  & $0.063$ & $1.0$  & $1.7$ & $1.0$ \\
      \hline
      CIFAR-100  & $0.0325$ & $0.1$  & $1.2$ & $1.67$ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}
#+END_EXPORT
** B Ablative and sensitive study results in Sarhan et al. (2020)

For ease of comparison, we include two Figures from the reference paper. All
rights for Figures [[fig:sarhan_ablative]] and [[fig:sarhan_sensitivity_adult]]
reserved by Sarhan et al.

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Figure 3 from \cite{sarhan2020fairness}, with original caption: /Ablative study. Dark gray and light gray dashed lines represent the accuracy results on the target and sensitive task respectively for the ``Entropy + KL Orth." model./
#+label: fig:sarhan_ablative
[[file:../figures/sarhan_ablative.png]]

#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+CAPTION: Figure 5 from \cite{sarhan2020fairness}, with original caption: /Sensitivity analysis on the Adult dataset/
#+label: fig:sarhan_sensitivity_adult
[[file:../figures/sarhan_sensitivity_adult.png]]

* References
