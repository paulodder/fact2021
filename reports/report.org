#+BIND: org-export-use-babel nil
#+TITLE: Report FACT 2021
#+AUTHOR: Jeroen Jagt, Paul Lodder, Pim Meerdink, Siem Teusink
#+EMAIL: <paul_lodder@live.nl>
#+DATE: January 8, 2021
#+LATEX: \setlength\parindent{0pt}
#+LATEX_HEADER: \usepackage{subcaption}
#+LaTeX_HEADER: \usepackage[]{neurips_2019}
#+LaTeX_HEADER: \usepackage[utf8]{inputenc} % allow utf-8 input
#+LaTeX_HEADER: \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
#+LaTeX_HEADER: \usepackage{hyperref}       % hyperlinks
#+LaTeX_HEADER: \usepackage{url}            % simple URL typesetting
#+LaTeX_HEADER: \usepackage{booktabs}       % professional-quality tables
#+LaTeX_HEADER: \usepackage{amsfonts}       % blackboard math symbols
#+LaTeX_HEADER: \usepackage{nicefrac}       % compact symbols for 1/2, etc.
#+LaTeX_HEADER: \usepackage{microtype}      % microtypography
#+LaTeX_HEADER:
#+LaTeX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LaTeX_HEADER: \usepackage[normalem]{ulem}
#+LaTeX_HEADER: \newif{\ifhidecomments}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session report :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex
#+BIBLIOGRAPHY: refs plain

* settings :noexport:
#+BEGIN_SRC emacs-lisp :exports none
(setq org-export-with-toc nil)
(setq org-export-with-section-numbers nil)
;; (setq org-export-latex-hyperref-format "\\ref{%s}")

(package-initialize)
(use-package ox-latex-subfigure
  :init
  (setq org-latex-prefer-user-labels t)
  :load-path "~/Dropbox/ProjectWeekends/lisp/ox-latex-subfigure/"
  :config (require 'ox-latex-subfigure))

(require 'org-ref)
(setq org-ref-default-bibliography "refs.bib")
;; (setq org-latex-pdf-process (list "latexmk -pdf %f -shell-escape"))

#+END_SRC

#+RESULTS:
: refs.bib

* Reproducibility Summary
Summary can be a page long max and needs to be included
** Scope of Reproducibility
** Methodology
** Results
** What was easy
** What was difficult
** Communication with original authors

\newpage
* 1 Introduction
The work of [cite Sarhan] introduces a new way of learning representations that
are useful for a specified task while robust against sensitive attributes. They
propose a method that decomposes the latent representation into a target and
sensitive representation. The representations are made to be orthogonal to
prevent sensitive information leaking into the target
representation. Furthermore, they introduce the disentanglement property to
"split the generative factors of each learned code" [cite Sarhan]. Last, they
ensure that the target representation is agnostic to the sensitive information
using a  maximum entropy approach.

* 2 Scope of reproducibility
The reference work by Sarhan et al works towards a method of generating
embeddings of data, while remaining invariant towards a particular
feature. This is done by explicitly learning an encoding of the sensitive
information, and forcing this encoding to be orthogonal and disentangled with
respect to the actual 'target' embeddings that are to be used in the downstream
task.

[cite Sarhan] aim to answer two questions with their experiments. 1) how does
the learned target representation perform when predicting target attributes,
and 2) how much information of the sensitive attributes is still present in the
learned target representation? To answer these questions they report a /target
accuracy/ and a /sensitive accuracy/, which we define in section
[[#sec:exp-setup]]. The authors' reported values can be found in Table
[[tab:sarhan_accuracies]]. Note that some of the accuracies were not reported
exactly and have therefore been deduced from figures. It is our aim to
reproduce the accuracies found by the authors.


#+ATTR_LATEX: :width 0.8\linewidth :float nil
#+attr_latex: :align c|c|c
#+CAPTION: Accuracies aquired by [cite Sarhan]. \textbf{Accuracies in bold} were not reported exactly and have been deduced from a figure.
#+label: tab:sarhan_accuracies
|-----------+-----------------+--------------------|
| dataset   | target accuracy | sensitive accuracy |
|-----------+-----------------+--------------------|
| Adult     | $\bm{0.68}$     | $0.6826$           |
| German    | $\bm{0.77}$     | $0.71$             |
| CIFAR-10  | $0.9725$        | $0.1907$           |
| CIFAR-100 | $0.7074$        | $0.1447$           |
| YaleB     | $\bm{0.92}$     | $\bm{0.52}$        |
|-----------+-----------------+--------------------|

Furthermore, the authors conduct an ablative study, in which specific parts of
the loss of the model we excluded, in order to observe the models behaviour and
understand the role of each of the loss terms within the training process. For
example, Sarhan et al remove the orthogonality constraint of the learned
sensitive and target embeddings by removing a loss term. Performance on the
same datasets as lised above in table [[tab:sarhan_accuracies]] is reported. In
total, the authors describe 6 variants of the model in this section. As such,
we will attempt to reproduce these results, aswell.

Finally, the authors preform a sensitivity analysis on the parameters that
weigh the different loss terms. In particular the final target accuracy and
sensitivity accuracy are reported as a function of the weight assigned to the
entropy term, and the KL term. These results are displayed as a
heatmap. Besides this two terms that control the decay of these loss terms are
also varied and the final sensitivity and target accuracy reported in a
heatmap.

* 3 Methodology
The code of the author's implementation of this project was not
available. Therefore, it was our goal to reproduce the implementation from the
description in the paper. The essential elements of the model are described in
the next section.
** 3.1 Model descriptions
Let $\mathcal{X}$ be the dataset and let $\bm{x} \in \mathbb{R}^D$ be a single
input sample. Each sample has an associated target vector $\bm{y} \in
\mathbb{R}^n$ and an associated sensitive attribute vector $\bm{s} \in
\mathbb{R}^m$, with $n$ and $m$ classes respectively. The aim is to create two
latent representations; a target latent representation $\bm{z}_T$ and a
sensitive latent representation $\bm{z}_S$. The encoder has the following shape:
The first part of the encoder $f(\bm{x}, \theta)$ can be shared between the target
and sensitive representation, depending on the dataset. After a data sample
$\bm{x}$ is fed through said shared encoder, the result is fed through two separate encoders,
$q_{\theta_T}(\bm{z}_T | \bm{x})$ and $q_{\theta_S}(\bm{z}_S | \bm{x})$, to
create the target and sensitive representations respectively. The target and
sensitive encoders are parameterized by $\theta_T$ and $\theta_S$
respectively. The shared part of the encoder is therefore paramaterized by
$\theta = \theta_T \cap \theta_S$.
Each representation is then used as input for the corresponding discriminator,
either the target discriminator $q_{\phi_T}(\bm{z}_T | \bm{x})$ or the
sensitive discriminator $q_{\theta_S}(\bm{z}_S | \bm{x})$.

- TO-DO: maybe make use different letter (r? d?) to indicate discriminator
  probability as it is confusing now.

The encoder and discriminator are trained in supervised fashion to minimize the
following losses, which combined we call the representation loss:
\begin{align}
\label{eq:recon-losses}
\mathcal{L}_{T}(\theta_{T},\phi_{T}) &= KL(p(\bm{y}|\bm{x})\parallel
q_{\phi_{t}}(\bm{y}|\bm{z}_{T})) \\
\mathcal{L}_{S}(\theta_{S}^{*},\phi_{S}) &= KL(p(\bm{s}|\bm{x})\parallel
q_{\phi_{S}}(\bm{y}|\bm{z}_{S}))
\end{align}

Here $\theta_S^* = \theta_S \backslash \theta$. These losses are effectively
equal to the cross-entropy between the predicted values for the targets and
sensitive attributes and their actual values.

To ensure no sensitive info leaking into the target representation, we
maximimze "the entropy of the sensitive discriminator given the target
representation". This is achieved by minimising
\begin{equation}
\label{eq:entropy-loss}
\mathcal{L}_{E}(\phi_{S},\theta_{T}) =
KL(q_{\phi_S}(\bm{s}|\bm{z}_{T})\parallel\mathcal{U}(\bm{s}))
\end{equation}

Last, we want to ensure that there is some level of independence between the
two representations, i.e., ideally the posterior $p(\bm{z}_T | \bm{x})$ would be
statistically independent of $p(\bm{z}_S | \bm{x})$. Following [cite Sarhan] we
relax this independence requirement to enforcing 1) a disentaglement property
and 2) orthogonality between the two representations. To enforce these
properties, we need to /estimate/ the aforementioned posteriors (as they are
intractable) using Variational Inference. The encoder network will be similar
to the encoder of a VAE-like model [cite VAE] that outputs the means and
variances, $\bm{\mu}_T$ and $\bm{\sigma}_T$ for both representations. To
enforce the disentangelement property we minimize the KL-divergence between the
output posterior $q_{\theta_T} (\bm{z}_T | \bm{x})$ and some prior $p (\bm{z}_T)$:

\begin{align}
\label{eq:od-losses}
\mathcal{L}_{z_{T}}(\theta_{T}) &= KL(q_{\theta_{T}}(\bm{z}_{T} \vert \bm{x}) \parallel
  p(\bm{z}_{T}))
\end{align}

Here $q_{\theta_T} (\bm{z}_T | \bm{x}) = \mathcal{N} (\bm{z}_T | \bm{\mu}_T,
\text{diag} (\bm{\sigma}_T ^2))$ and $p (\bm{z}_T) = \mathcal{N} (\bm{0} ,
\bm{I})$. We can construct a similar KL-divergence term for the sensitive
representations. To enforce the orthogonality between the two representations
we can make sure that the means of the prior distributions are orthogonal. This
will indirectly push the posterior distributions to be orthogonal.

We call these losses combined the /Orthogonal Disentangled/, or /OD/ loss.
$$
\mathcal{L}_{OD}(\theta_{T}, \theta_S) = \mathcal{L}_{z_{T}}(\theta_{T})  +
\mathcal{L}_{z_{S}}(\theta_{S})
$$
We can use the re-parameterization trick [cite reparam trick] to sample
from the posterior distribution to obtain the latent representations, which can
then be fed to the respective discriminators.

We combine all of the aforementioned losses and the objective becomes to
minimize this combined loss:
\begin{equation}
\label{eq:total-loss}
\underset{\theta_{T},\theta_{S},\phi{T},\phi{S}}{argmin}
\mathcal{L}_{T}(\theta_{T},\phi_{T}) +
\mathcal{L}_{S}(\theta_{S^{*}},\phi_{S}) \lambda_{E}\mathcal{L}_{E}(\theta_{T},
\phi_{S})  + \lambda_{OD}\mathcal{L}_{OD}(\phi_{T},\phi_{S})
\end{equation}

Here $\lambda_{OD}$ and $\lambda_E$ are weights of the OD loss and the maximum
entropy loss respectively. Additionally, we introduce two decay parameters,
$\gamma_{OD}$ and $\gamma_{E}$ which allows us to change the weights of the
aforementioned losses while training. The OD loss weight at epoch $t$ during
training will be calculated as follows:
\begin{equation}
\lambda_{OD}^{(t)} = \lambda_{OD}^{(0)} \gamma_{OD}^{t/t_s}
\end{equation}
Here $t_s$ is the so-called /step-size/ parameter, and $\lambda_{OD} is the
initial OD loss weight. The maximum entropy loss weight will be computed in the
same way. $\lambda_{OD}^{(0)}, \lambda_{E}^{(0)}, \gamma_{OD},
\gamma_{E}$ and $t_s$ are all hyperparamters that we need to set. More
information on those will follow.

** 3.2 Datasets
In order to reproduce the results obtained by Sarhan et al. it was necessary to
apply the model to five datasets. Below, we outline some basic properties of
the datasets, aswell as outlining steps taken in the pre processing, and we
explain the sensitive and target attributes that are to be modeled.

*** Adult
The adult dataset was collected from the UCI machine learning repository
cite:uci. The dataset contains census data, and the task is to predict whether
a persons income exceeds 50K. In total, it contains 48842 rows and 14
attributes. These attributes are a mix of categorical and continuous and
contain information about the persons gender, education and occupation. The
categorical columns were one-hot encoded, and the continuous variables were
left unchanged. In the end, this left us with 108 features. The sensitive
attribute for this dataset is the gender. All missing values occurred in
categorical columns, these were explicitly encoded as a seperate
category. Approximately 75% of the rows' income did not exceed 50K, and around
67% are male. The train/test split is explicitly defined in the repository
through aving seperate train and test files, the train:test ration is 2:1.
*** German
The German dataset was also collected from the UCI machine learning repository
cite:uci. Again, the dataset contains a mix of categorical and continuous
attributes. It contains 1000 rows and 20 features. Information pertaining to
the occupation, age and personal status are included, the task is to classify
rows as having good or bad credit risk. Similar to the adult dataset, the
categorical features were one hot encoded, and continuous columns left
unchanged, the sensitive attribute is gender. There are no missing values in
the dataset. After our preprocessing, the dataset contained 61 columns. We
randomly split the data with a train:test ration of 4:1. 68% of the rows in the
dataset are male, and 70% have bad credit risk.

*** YaleB
The YaleB dataset was collected from the University of Toronto computer science
department website cite:GeBeKr01. The dataset we used contains 2433 grayscale,
168x192 images of 38 human faces under different lighting conditions.  The
cropped variant was used, the task is to classify images according to the
subjects of the images. We constructed a sensitive attribute by clustering the
illumination conditions into 5 classes loosely corresponding to top left,
bottom left, top right, bottom right and center. We defined these classes
ourselves as we were unable to find detailed information on how this was done
in the study by Sarhan et al. Our sensitive attributes' distribution was not
skewed, with the 'center' class containing around 800 images, and the between
340 and 380. This was not in line with the paper by Sarhan et al, who mention
that a majority class classifier could attain 50% accuracy, in our case this is
around 0.35. Unfortunately, we were unable to find sufficient information to be
able to replicate the ratios mentioned in the reference paper, and instead
constructed our own sensitive attributes.

Our training dataset was comprised of 190 images, just like Sarhan et al. It is
important to note that our testing dataset contained 2243 images, while the
original study's dataset contained only 1096. The reason for this is unclear as
we used the full dataset, and found no mention of omitting images in the paper
by reference paper. The images were flattened into vectors of length 32256. The
target feature was evenly distributed across the dataset, i.e. the dataset
contained 64 images of each person.

*** cifar 10
The cifar 10 dataset was also collected from the University of Toronto computer
science department website cite:GeBeKr01. It consists of 60 000 32x32 colour
images that are divided into 10 classes such as airplane, automobile and
bird. For our purposes, we construct a new target attribute, one that denotes
whether the subject of the image is alive or not. The sensitive attribute,
then, is the original label of the image. We must thus learn a representation
of the image that allows a classifier to identify whether the subject is alive
or not, while remaining invariant to the actual class of the image. There are
6000 images of each original class label, and due to the nature of these
classes 60% are alive, and 40% are not. Due to the convolutional nature of the
network that we used for this dataset, the images were not flattened and
instead the 32x32x3 images were fed through our network directly. The
train:test ratio was 5:1, the provided split was used.

*** cifar 100
Finally, the cifar 100 dataset was also collected from the University of
Toronto computer science department website cite:GeBeKr01. It is similar to
CIFAR-10, except it contains 100 classes of 600 images each. Similarly to
CIFAR-10, the 100 classes in CIFAR-100 are split into 20 coarses classes that
cluster similar concepts into one category. For example: 'beaver'. 'dolphin'
and 'otter' all belong to the coarse class 'aquatic mammals'. More details on
this split can be found at cite:proteek. The task is the prediction of the
coarse class while remaining invariant to the finer class. As mentioned before,
there are 600 images of each fine class (i.e. the original labels), and 3000
images of each coarse class (i.e. our targets). Again, a convolutional neowrk
was used so no flattening was applied to our images. The provided train/test
split was used, the train test ratio was therefore 5:1.

** 3.3 Implementation details
Following the paper of [Sarhan], we implement the following networks for the
several datasets.
(Maybe better to put all of these in tables and give only short description in
paragraphs?)
*** Adult and German
For the Adult and German dataset we implement a MLP with one hidden layer as
encoder and a MLP with two hidden layers as discriminators. The two
discriminators are followed by a logistic regression layer to make a
prediction. All hidden layers contain 64 units. Last, the latent
representations have size 2.
*** YaleB
For the extended YaleB dataset we implement a MLP with one hidden layer as
encoder and a MLP with two hidden layers as discriminators. The two
discriminators are followed by a linear layer to make a prediction. All hidden
layers contain 100 units. Last, the latent representations have size 2 (?).
*** CIFAR
For the CIFAR datasets we implement a ResNet-18 [cite resnet] architecture for
the encoder. For the discriminator and the target classifier we use an MLP with
two hidden layers, with 256 and 128 neurons. (z size?? maybe in resnet 18
paper)

** 3.4 Hyperparameters
Most used hyperparameters were taken directly from the supplement provided by
Sarhan et al. Optimal values for quote some hyperparameters were not reported,
and as a result we empirically set these to values that seemed to result in
satisfactory performance.

In particular, the amount of epochs that the model was trained was not reported
in either the paper or it's supplementary materoa;, this was quite an important
value given that no explicit stopping criterion was mentioned, either. In
correspondene with Sarhan, we were able to set values for the step_size
hyperparameter that correspond to those used by the original team. Furthermore,
amongst the not reported hyperparameters were those involved the training of
the MLP target and sensitive predictors. These include the optimizer used, the
learning rate, weight decay, amount of epochs aswell as the nonlinearities, to name a
few.


** 3.5 Experimental setup and code
:PROPERTIES:
:CUSTOM_ID: sec:exp-setup
:END:
*** Setup Reproducibility
*** Evaluation
Evaluation of the embeddings learned by our model is non trivial, as we must
gather whether the embeddings adequately represent the data for the downstream
task (e.g. classification), while also ensuring that the embeddings contain no
sensitive information. In order to quantitively evaluate our model after
completing training, we train two classifiers. These classifiers use the test
data that is embedded using our trained model in the target space.

The first classifier, known as the /target predictor/ is trained to predict the
target label from the target embeddings. In accordance with the reference
paper, we evaluated the target predictor using accuracy as metric. It is
desirable that the target predictor performs as well as possible, as this means
that the target embeddings embed the information necessary for the downstream
task well.

The second classifier, known as the /sensitive predictor/ is trained to predict
the sensitive attribute from the target label. It is desirable that this
classifier preforms poorly, as we would like there to be no information
pertaining to the sensitive attribute in our target embedding. As such, we
would like the model to be as close to a 'majority classifier' as possible,
where the model is forced to simply predict the majority label for each data
row as it has no meaningful information with which to make a prediction about
the sensitive attribute. Again, we use solely accuracy as evaluation metric.

** 3.6 Computational Requirements
- Include Hardware used (CPU/GPU used)
- For each model, include average run-time
- Include /total hours of GPU time/

* 4 Results
- High-level overview. Do our results support the paper's claims? Keep
  discussion for next section.
** 4.1 Results reproducing original paper
*** Accuracies Adult, Yaleb and German
#+LATEX_HEADER: \usepackage{subcaption}
#+NAME: fig:adult
#+CAPTION: Target and sensitive accuracies of our model compared with using the raw data and VAE embeddings for adult
#+ATTR_LATEX: :environment subfigure :width 0.5\textwidth :align c
| [[../figures/adult_target.png]] | <<fig:adult_target>> sensitive accuracy |
| target accuracy             | [[../figures/adult_sens.png]]               |
#+LATEX_HEADER: \usepackage{subcaption}

#+NAME: fig:adult
#+CAPTION: Target and sensitive accuracies of our model compared with using the raw data and VAE embeddings for german
#+ATTR_LATEX: :environment subfigure :width 0.5\textwidth :align c
| [[../figures/german_target.png]] | <<fig:german_target>> sensitive accuracy |
| target accuracy              | [[../figures/german_sens.png]]              |

#+LATEX_HEADER: \usepackage{subcaption}

#+NAME: fig:adult
#+CAPTION: Target and sensitive accuracies of our model compared with using the raw data (LR) for YaleB
#+ATTR_LATEX: :environment subfigure :width 0.5\textwidth :align c
| [[../figures/yaleb_target.png]] | <<fig:yaleb_target>> sensitive accuracy  |
| target accuracy             | [[../figures/yaleb_sens.png]]               |



Figures [[fig:adult]] show the accuracy for the target and sensitive predictors on
the adult dataset. The black line indicates the accuracy that a majority class
classifier would attain. The bars denoted by X correspond to direct use of the
input data for our target prediciton. Furthermore, a VAE was trained on the
adult and german datasets using MSE loss as reconstruction loss. These learned
embeddings were also used as feature for our target and sensitive
predictors. These accuracies correspond to the bars denoted with 'VAE'. For
YaleB, besides the model in question, logistic regression was also used on the
raw data to predict the sensitive and target attributes. This is denoted by

'LR'. In general, our results for adult and german are similar to those
obtained by Sarhan et al, while the results for yaleb are highly dissimilar:
our target predictor atains an accuracy of around 20 percent, Sarhan et al
attained around 90.

'LR'.
*** Ablative
#+BEGIN_SRC sh
# adult
bash scripts/ablative.sh adult
python scripts/visualize_ablative.py -d adult
# german
bash scripts/ablative.sh german
python scripts/visualize_ablative.py -d german
# yaleb
bash scripts/ablative.sh yaleb
python scripts/visualize_ablative.py -d yaleb
# cifar10
bash scripts/ablative.sh cifar10
python scripts/visualize_ablative.py -d cifar10
# cifar100
bash scripts/ablative.sh cifar100
python scripts/visualize_ablative.py -d cifar100
#+END_SRC


#+LATEX_HEADER: \usepackage{subcaption}
#+NAME: fig:ablative
#+LABEL: fig:ablative
#+CAPTION: Target and sensitive accuracies of our model compared with using the raw data and VAE embeddings for german
#+ATTR_LATEX: :environment subfigure :width 0.5\textwidth
| [[../figures/ablative.adult.png]]    | <<fig:ablative_adult>> Adult       |
| [[../figures/ablative.german.png]]   | <<fig:ablative_german>> German     |
| [[../figures/ablative.yaleb.png]]    | <<fig:ablative_yaleb>> YaleB       |
| [[../figures/ablative.cifar10.png]]  | <<fig:ablative_cifar10>> Cifar10   |
| [[../figures/ablative.cifar100.png]] | <<fig:ablative_cifar100>> Cifar100 |
|                                  |                                    |


** 4.2 Results beyond original paper
- Report additional results we acquired, if relevant.

* 5 Discussion
- Discuss whether we think our results support the claims of the paper. Discuss
  strengths and/or weaknesses of our approach.
** 5.1 What was easy
** 5.2 What was difficult
** 5.3 Communication with original authors
* References
